{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Jax\n",
    "subtitle: The New Hot Thing\n",
    "format:\n",
    "  revealjs: \n",
    "    slide-number: true\n",
    "    chalkboard: \n",
    "      buttons: false\n",
    "    preview-links: auto\n",
    "    logo: images/logo.png\n",
    "    css: styles.css\n",
    "    footer: <https://github.com/google/jax>\n",
    "resources:\n",
    "  - dascore.pdf\n",
    "---"
   ],
   "id": "8053537d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax: What is it? \n",
    "\n",
    ":::{.incremental}\n",
    "- Python library\n",
    "- Numpy replacement\n",
    "- Linear algebra workhorse\n",
    "  - Code complied to XLA \n",
    "  - Supports CPU, GPU, TPU\n",
    "- Set of functional transforms\n",
    "- Set of deep learning primitives\n",
    "- How can we use it in geophysics?\n",
    ":::\n",
    "\n",
    "\n",
    "## Jax: Grad\n",
    "\n",
    "Jax supports automatic differentiation:\n",
    "\n",
    ":::{.incremental}\n",
    "- python control structures\n",
    "- recursion\n",
    "- forward or backward\n",
    ":::\n",
    "\n",
    ":::{.fragment}\n"
   ],
   "id": "fad404a1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#| echo: true\n",
    "from jax import grad\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def tanh(x):  # Define a function\n",
    "  y = jnp.exp(-2.0 * x)\n",
    "  return (1.0 - y) / (1.0 + y)\n",
    "\n",
    "grad_tanh = grad(tanh)  # Obtain its gradient function\n",
    "print(grad_tanh(1.0))   # Evaluate it at x = 1.0"
   ],
   "id": "3c7023af",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    "## Jax: Just in Time (jit) Compilation\n",
    "\n",
    "<br>\n"
   ],
   "id": "6fba56ad"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#| echo: true\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "def slow_f(x):\n",
    "  # Element-wise ops see a large benefit from fusion\n",
    "  return x * x + x * 2.0\n",
    "\n",
    "fast_f = jit(slow_f)\n",
    "\n",
    "x = jnp.ones((5000, 5000))\n",
    "\n",
    "%timeit -n10 -r3 fast_f(x) \n",
    "%timeit -n10 -r3 slow_f(x)"
   ],
   "id": "1a968713",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax as Numpy\n",
    "\n",
    "```python\n",
    "import jax.numpy as jnp\n",
    "\n",
    "ar = jnp.array([1,2,3])\n",
    "```"
   ],
   "id": "9696c17e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
