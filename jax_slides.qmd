---
title: Jax
subtitle: The New Hot Thing
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: images/logo.png
    css: styles.css
    footer: <https://github.com/google/jax>
resources:
  - dascore.pdf
---

## Jax: What is it? 

:::{.incremental}
- Python library from DeepMind
- Linear algebra workhorse
  - Code complied to XLA 
  - Supports CPU, GPU, TPU
  - Supports distributed devices (many GPU/TPU)
- Set of functional transforms
- Set of deep learning primitives
- How can we use it in geophysics?
:::



## Jax: Design and Ecosystem

:::{.incremental}
- Heavily functional (Haskell type signatures)
  - function transformations/compositions
  - immutable data structures
- jax.lax - XLA mapped primitives
- jax.numpy - near numpy parity 
- [50+ jax-based libraries](https://github.com/n2cholas/awesome-jax)
:::


## Jax: Grad

Jax supports automatic differentiation:

:::{.incremental}
- python control structures
- recursion
- forward or backward
:::

:::{.fragment}

```{python}
#| echo: true
from jax import grad
import jax.numpy as jnp

def tanh(x):  # Define a function
  y = jnp.exp(-2.0 * x)
  return (1.0 - y) / (1.0 + y)

grad_tanh = grad(tanh)  # Obtain its gradient function
print(grad_tanh(1.0))   # Evaluate it at x = 1.0
```
:::



## Jax: Grad

Grad is composable!

<br>

```python
from jax import jacfwd, jacrev

def hessian(f):
    return jacfwd(jacrev(f))

H = hessian(lambda x, y, z: x**2 + y**2 + z**2)
```


## Jax: V map

Advanced controls for broadcasting

```{python}
import jax.numpy as jnp
from jax import vmap

add = jnp.add

a = jnp.array([1, 2, 3, 4])
b = jnp.array([[5, 6], [7, 8]])

add_loop = lambda a, b: jnp.array([add(x, b) for x in a])

add_no_loop = vmap(add, in_axes=(0, None), out_axes=0)

out1 = add_loop(a, b)
out2 = add_no_loop(a, b)

print(out1)
print(out2)
```


## Jax: Just in Time (jit) Compilation

<br>

```{python}
#| echo: true
import jax.numpy as jnp
from jax import jit

def slow_f(x):
  # Element-wise ops see a large benefit from fusion
  return x * x + x * 2.0

fast_f = jit(slow_f)

x = jnp.ones((5000, 5000))

%timeit -n10 -r3 fast_f(x) 
%timeit -n10 -r3 slow_f(x)
```

## Jax as Numpy

Simply import jax.numpy

<br>

```python
import jax.numpy as jnp

ar = jnp.array([1,2,3])
```

## Jax as Numpy

No pseudo-random state!

<br>

```python
import jax

seed = 42
key = jax.random.PRNGKey(seed)
ar = jax.random.uniform(key, (100, 1000))
```

## Jax as Numpy

Arrays are immutable

<br>

```python
import jax.numpy as jnp

ar = jnp.array([1,2,3])
ar[:1] = jnp.array([0,0])  # WRONG!
new = ar.at[:2].set([0,0])
```

## Jax as Numpy

<br>

There are other [sharp bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions) 


## Jit timing

Timing functions:

<br>

```python
def axpy_self(ar, scalar):
    return ar * scalar + ar

def pow_add(ar):
    return ar + ar**2 + ar**3 + ar**4
```


## Jit timing
CPU 

::: {layout-ncol=2}
![axpy no jit](fig/cpu/axpy_no_jit.png)

![axpy jit](fig/cpu/axpy_jit.png)
:::


## Jit timing
CPU 

::: {layout-ncol=2}
![power add no jit](fig/cpu/power_add_no_jit.png)

![power add jit](fig/cpu/power_add_jit.png)
:::


## Jit timing
GPU 

::: {layout-ncol=2}
![axpy no jit](fig/gpu/axpy_no_jit.png)

![axpy jit](fig/gpu/axpy_jit.png)
:::


## Jit timing
GPU

::: {layout-ncol=2}
![power add no jit](fig/gpu/power_add_no_jit.png)

![power add jit](fig/gpu/power_add_jit.png)
:::



## 2D acoustic wave equation

  - [Multiple TPU](https://github.com/google/jax/blob/main/cloud_tpu_colabs/Wave_Equation.ipynb)
  - [j-Wave: Differentiable acoustic simulations in JAX](https://github.com/ucl-bug/jwave)
